# Scalable Q&A System with LangChain and ChromaDB

This project showcases a scalable Question & Answer (Q&A) system that enables natural language querying across large unstructured documents. It integrates LangChain for orchestration, ChromaDB for efficient vector storage, and Huggingface embeddings for semantic understanding. The system is optimized for real-time performance and cost-efficient processing at scale.

## ðŸš€ Features

- ðŸ”Ž **Natural Language Querying**: Ask plain English questions and get accurate, context-aware answers.
- ðŸ“š **Large Document Support**: Handles multi-format datasets, including URLs and CSVs.
- ðŸ§  **State-of-the-Art Embeddings**: Utilizes Huggingface embeddings for deep semantic indexing.
- âš¡ **Real-Time Information Retrieval**: Fast responses through live vector search and context-aware retrieval.
- ðŸ“¦ **Optimized Storage**: Efficient tokenization and storage using ChromaDB for low-latency search.
- ðŸ’¡ **Cost-Efficient Scaling**: Powered by MosaicMLâ€™s MPT-30B models to reduce computational overhead on large datasets.

## ðŸ›  Architecture Overview

```plaintext
[Data Sources: URLs, CSVs]
        â†“
[Document Processing Pipeline]
  - Tokenization
  - Embedding with Huggingface
        â†“
[Vector Storage in ChromaDB]
        â†“
[LangChain Q&A Interface]
  - Query Embedding
  - Similarity Search
  - Answer Generation
        â†“
[Real-Time Response]
